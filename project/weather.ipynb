{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fc45c940",
   "metadata": {},
   "source": [
    "Analysis the wind speed around the country with a view to windfarms\n",
    "\n",
    "• You may look for your own source of historic weather information, and/or use the Met Eireann one (Historical Data - Met Éireann - The Irish Meteorological Service). Click on the download button to get a zip file that contains a CSV file.\n",
    "• You may need to clean and normalize the data before doing analysis\n",
    "• Questions you can ask:\n",
    "o How much wind power is there at a particular location?\n",
    "▪ This is quite open ended, is this just the mean wind speed for an hour/day/month/year, or should you take into account that\n",
    "there are wind ranges that the windfarms can operate in. (min max speeds)\n",
    "▪ Some analysis of what power when would be useful (time of day/year)\n",
    "o Are the wind speeds likely to be the same in 10 years in the future? ie is there a trend in recorded wind speeds over the last few decades.\n",
    "o Is there any other weather metric worth analyzing (eg rain, temp)\n",
    "o What will the power output of the windfarms in Ireland be like next week, according to the weather forecasts? (ok that is a tricky one, because you would need to get, or make up, information about the size and locations of the wind farms in Ireland, one find/makeup the windspeed to power output equation.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b13c21f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "efad8ec2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5193d89d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def file_with_station(file,station):\n",
    "\n",
    "    # Read file as text\n",
    "    with open(file, \"r\", encoding=\"utf-8\") as f:\n",
    "        lines = f.readlines()\n",
    "\n",
    "    # Find the row number where the data starts i.e. the row that contain 'date' in its first column\n",
    "    header_row = None\n",
    "\n",
    "    for i, line in enumerate(lines):\n",
    "        # split on comma and strip spaces\n",
    "        first_cell = line.split(\",\")[0].strip().lower()\n",
    "        if first_cell == \"date\":\n",
    "            header_row = i\n",
    "            break\n",
    "    \n",
    "    # Read file as csv, delete uneccessary rows\n",
    "    df = pd.read_csv(file, skiprows=header_row,low_memory=False)\n",
    "    # modify the format of 'date'\n",
    "    df['date'] = pd.to_datetime(df['date'], format='%d-%b-%Y %H:%M')\n",
    "    # Add a column which takes the station name as value\n",
    "    df[\"station\"]= station\n",
    "    # Add a column that contains only date details\n",
    "    df['dateonly']= df['date'].dt.date\n",
    "    # Add a column that contains only month-year\n",
    "    df['yearmonth'] = pd.to_datetime(df['date']).dt.strftime('%Y-%m')\n",
    "    # Add a column that contains only month\n",
    "    df['month'] = pd.to_datetime(df['date']).dt.strftime('%m')\n",
    "    # Save as a new file in folder stationdata\n",
    "    df.to_csv(f\"stationdata/{station}.csv\")\n",
    "    print(f\"The file {station}.csv is now created.\")\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f9322076",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The file MACE HEAD.csv is now created.\n"
     ]
    }
   ],
   "source": [
    "file_with_station(\"data/hly275.csv\",\"MACE HEAD\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "208056f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The file MACE HEAD.csv is now created.\n",
      "The file OAK PARK.csv is now created.\n",
      "The file SHANNON AIRPORT.csv is now created.\n",
      "The file DUBLIN AIRPORT.csv is now created.\n",
      "The file MOORE PARK.csv is now created.\n",
      "The file BALLYHAISE.csv is now created.\n",
      "The file SHERKIN ISLAND.csv is now created.\n",
      "The file MULLINGAR.csv is now created.\n",
      "The file ROCHES POINT.csv is now created.\n",
      "The file NEWPORT.csv is now created.\n",
      "The file DUNSANY.csv is now created.\n",
      "The file GURTEEN.csv is now created.\n",
      "The file MALIN HEAD.csv is now created.\n",
      "The file JOHNSTOWN CASTLE 2.csv is now created.\n",
      "The file ATHENRY.csv is now created.\n",
      "The file MT DILLON.csv is now created.\n",
      "The file FINNER.csv is now created.\n",
      "The file CLAREMORRIS.csv is now created.\n",
      "The file VALENTIA OBSERVATORY.csv is now created.\n",
      "The file BELMULLET.csv is now created.\n",
      "The file CASEMENT.csv is now created.\n",
      "The file CORK AIRPORT.csv is now created.\n",
      "The file KNOCK AIRPORT.csv is now created.\n"
     ]
    }
   ],
   "source": [
    "file_with_station(\"data/hly275.csv\",\"MACE HEAD\")\n",
    "file_with_station(\"data/hly375.csv\",\"OAK PARK\")\n",
    "file_with_station(\"data/hly518.csv\",\"SHANNON AIRPORT\")\n",
    "file_with_station(\"data/hly532.csv\",\"DUBLIN AIRPORT\")\n",
    "file_with_station(\"data/hly575.csv\",\"MOORE PARK\")\n",
    "file_with_station(\"data/hly675.csv\",\"BALLYHAISE\")\n",
    "file_with_station(\"data/hly775.csv\",\"SHERKIN ISLAND\")\n",
    "file_with_station(\"data/hly875.csv\",\"MULLINGAR\")\n",
    "file_with_station(\"data/hly1075.csv\",\"ROCHES POINT\")\n",
    "file_with_station(\"data/hly1175.csv\",\"NEWPORT\")\n",
    "file_with_station(\"data/hly1375.csv\",\"DUNSANY\")\n",
    "file_with_station(\"data/hly1475.csv\",\"GURTEEN\")\n",
    "file_with_station(\"data/hly1575.csv\",\"MALIN HEAD\")\n",
    "file_with_station(\"data/hly1775.csv\",\"JOHNSTOWN CASTLE 2\")\n",
    "file_with_station(\"data/hly1875.csv\",\"ATHENRY\")\n",
    "file_with_station(\"data/hly1975.csv\",\"MT DILLON\")\n",
    "file_with_station(\"data/hly2075.csv\",\"FINNER\")\n",
    "file_with_station(\"data/hly2175.csv\",\"CLAREMORRIS\")\n",
    "file_with_station(\"data/hly2275.csv\",\"VALENTIA OBSERVATORY\")\n",
    "file_with_station(\"data/hly2375.csv\",\"BELMULLET\")\n",
    "file_with_station(\"data/hly3723.csv\",\"CASEMENT\")\n",
    "file_with_station(\"data/hly3904.csv\",\"CORK AIRPORT\")\n",
    "file_with_station(\"data/hly4935.csv\",\"KNOCK AIRPORT\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "853f0e89",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_5683/2509365597.py:1: DtypeWarning: Columns (3,5,7,8,9,10,11,13,15) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv('stationdata/ATHENRY.csv')\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "np.False_"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('stationdata/ATHENRY.csv')\n",
    "pd.api.types.is_datetime64_any_dtype(df[\"date\"])\n",
    "df[\"date\"] = pd.to_datetime(df[\"date\"], errors=\"coerce\")\n",
    "pd.api.types.is_datetime64_any_dtype(df[\"date\"])\n",
    "df[\"date\"].isna().any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53db446d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "stationinfo.csv is ready\n",
      "datestudy.csv is ready\n"
     ]
    }
   ],
   "source": [
    "# Folder contains all files with stations in column\n",
    "DATADIR = Path(\"stationdata\")\n",
    "#STATIONINFO = \"stationinfo.csv\"\n",
    "STUDYDATETILL2025 = \"studydate.csv\"\n",
    "\n",
    "# Create a list 'stationlist'\n",
    "stationlist = []\n",
    "\n",
    "# In each file in folder 'stationdata'\n",
    "for file in DATADIR.glob('*.csv'):\n",
    "    # Read only 2 columns 'station' and 'date'\n",
    "    df = pd.read_csv(file, usecols=[\"station\", \"date\"])\n",
    "    # Remove rows where 'date' is missing\n",
    "    #df = df[df[\"date\"].notna()]\n",
    "    # Convert the column 'date' to datetime\n",
    "    df[\"date\"] = pd.to_datetime(df[\"date\"], errors=\"coerce\")\n",
    "\n",
    "    # Get the station's name by getting the first row value of column 'station'\n",
    "    station_name = df[\"station\"].iloc[0]\n",
    "    # Get the start date of the station\n",
    "    start_date = df[\"date\"].min().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "    # Get the end date of the station\n",
    "    end_date = df[\"date\"].max().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "    \n",
    "    # Write the infos above in 'stationlist'\n",
    "    stationlist.append({\n",
    "            \"station\": station_name,\n",
    "            \"startdate\": start_date,\n",
    "            \"enddate\": end_date\n",
    "        })\n",
    "# Save stationinfo.csv\n",
    "stationinfo_df = pd.DataFrame(stationlist)\n",
    "\n",
    "stationinfo_df.to_csv('stationinfo.csv', index=False)\n",
    "print(\"stationinfo.csv is ready\")\n",
    "\n",
    "\n",
    "# Step 2: Generate datestudy.csv: the idea behind this step is to set the same start date and end date for all stations, \n",
    "# to make the comparison simpler for later?\n",
    "\n",
    "# get the max start date among all stations\n",
    "max_start = stationinfo_df[\"startdate\"].max()\n",
    "# get the max end date among all stations\n",
    "max_end = stationinfo_df[\"enddate\"].max()\n",
    "\n",
    "# Data frame that contains all dates from max_start to max_end\n",
    "all_dates = pd.date_range(start=max_start, end=max_end, freq=\"h\")\n",
    "\n",
    "# Save to datetill2025.csv\n",
    "datestudy_df = pd.DataFrame({\"date\": all_dates})\n",
    "datestudy_df.to_csv('datestudy.csv', index=False)\n",
    "print(\"datestudy.csv is ready\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "bda3a526",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2567/3057289317.py:12: DtypeWarning: Columns (3,5,7,8,9,10,11,13,15) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  station_df = pd.read_csv(file)\n",
      "/tmp/ipykernel_2567/3057289317.py:12: DtypeWarning: Columns (13,15) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  station_df = pd.read_csv(file)\n",
      "/tmp/ipykernel_2567/3057289317.py:12: DtypeWarning: Columns (3,5,7,8,9,10,11,13,15) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  station_df = pd.read_csv(file)\n",
      "/tmp/ipykernel_2567/3057289317.py:12: DtypeWarning: Columns (7,8,9,10,11,13,15) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  station_df = pd.read_csv(file)\n",
      "/tmp/ipykernel_2567/3057289317.py:12: DtypeWarning: Columns (7,13,15) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  station_df = pd.read_csv(file)\n",
      "/tmp/ipykernel_2567/3057289317.py:12: DtypeWarning: Columns (7,13,15) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  station_df = pd.read_csv(file)\n",
      "/tmp/ipykernel_2567/3057289317.py:12: DtypeWarning: Columns (19) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  station_df = pd.read_csv(file)\n",
      "/tmp/ipykernel_2567/3057289317.py:12: DtypeWarning: Columns (7,9,10,19) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  station_df = pd.read_csv(file)\n",
      "/tmp/ipykernel_2567/3057289317.py:12: DtypeWarning: Columns (3,5,7,8,9,10,11,13,15) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  station_df = pd.read_csv(file)\n",
      "/tmp/ipykernel_2567/3057289317.py:12: DtypeWarning: Columns (3,5,7,8,9,10,11,13,15) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  station_df = pd.read_csv(file)\n",
      "/tmp/ipykernel_2567/3057289317.py:12: DtypeWarning: Columns (3,5,7,8,9,10,11,13,15) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  station_df = pd.read_csv(file)\n",
      "/tmp/ipykernel_2567/3057289317.py:12: DtypeWarning: Columns (5,7,8,9,11,13,15) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  station_df = pd.read_csv(file)\n",
      "/tmp/ipykernel_2567/3057289317.py:12: DtypeWarning: Columns (3,5,7,8,9,10,11,13) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  station_df = pd.read_csv(file)\n",
      "/tmp/ipykernel_2567/3057289317.py:12: DtypeWarning: Columns (15) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  station_df = pd.read_csv(file)\n",
      "/tmp/ipykernel_2567/3057289317.py:12: DtypeWarning: Columns (15) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  station_df = pd.read_csv(file)\n",
      "/tmp/ipykernel_2567/3057289317.py:12: DtypeWarning: Columns (3,5,7,8,9,10,11,13,15) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  station_df = pd.read_csv(file)\n",
      "/tmp/ipykernel_2567/3057289317.py:12: DtypeWarning: Columns (3,5,7,8,9,10,13,15) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  station_df = pd.read_csv(file)\n",
      "/tmp/ipykernel_2567/3057289317.py:12: DtypeWarning: Columns (3,5,7,8,9,10,11,13,15) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  station_df = pd.read_csv(file)\n",
      "/tmp/ipykernel_2567/3057289317.py:12: DtypeWarning: Columns (3,5,7,8,9,10,11,13,15) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  station_df = pd.read_csv(file)\n",
      "/tmp/ipykernel_2567/3057289317.py:12: DtypeWarning: Columns (3,7,8,9,10,13,15) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  station_df = pd.read_csv(file)\n",
      "/tmp/ipykernel_2567/3057289317.py:12: DtypeWarning: Columns (3,5,7,8,9,10,11,13,15) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  station_df = pd.read_csv(file)\n",
      "/tmp/ipykernel_2567/3057289317.py:12: DtypeWarning: Columns (3,5,7,8,9,10,11,13,15) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  station_df = pd.read_csv(file)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 station  startdate\n",
      "0                DUNSANY         13\n",
      "1              BELMULLET          1\n",
      "2     JOHNSTOWN CASTLE 2         34\n",
      "3            CLAREMORRIS        207\n",
      "4               CASEMENT          4\n",
      "5        SHANNON AIRPORT         12\n",
      "6         DUBLIN AIRPORT          0\n",
      "7           CORK AIRPORT          0\n",
      "8           ROCHES POINT         18\n",
      "9   VALENTIA OBSERVATORY         45\n",
      "10            BALLYHAISE         28\n",
      "11              OAK PARK          1\n",
      "12             MULLINGAR         13\n",
      "13            MOORE PARK          0\n",
      "14         KNOCK AIRPORT          0\n",
      "15             MACE HEAD         92\n",
      "16             MT DILLON         62\n",
      "17        SHERKIN ISLAND          5\n",
      "18               NEWPORT         76\n",
      "19            MALIN HEAD         52\n",
      "20                FINNER       7421\n",
      "21               ATHENRY         50\n",
      "22               GURTEEN         35\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2567/3057289317.py:12: DtypeWarning: Columns (3,5,7,8,9,10,11,13,15) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  station_df = pd.read_csv(file)\n"
     ]
    }
   ],
   "source": [
    "stationinfo = pd.read_csv(\"stationinfo.csv\")\n",
    "# get the max start date among all stations\n",
    "max_start = stationinfo[\"startdate\"].max()\n",
    "\n",
    "DATADIR = Path(\"stationdata\")\n",
    "\n",
    "# Create a list\n",
    "sta_miss = []\n",
    "\n",
    "for file in DATADIR.glob('*.csv'):\n",
    "    # Read stations files\n",
    "    station_df = pd.read_csv(file)\n",
    "\n",
    "    # Keep only rows with dates in studydate.csv\n",
    "    station_df = station_df[station_df[\"date\"]>= max_start]\n",
    "    \n",
    "    # change the type on 'wdsp' to numeric, the missing value will be turned into NaN with parameter errors=\"coerce\"\n",
    "    station_df[\"wdsp\"] = pd.to_numeric(station_df[\"wdsp\"], errors=\"coerce\")\n",
    "\n",
    "    # count the number of missing values\n",
    "    station_df[\"wdsp\"].isna().sum()\n",
    "\n",
    "    # Skip stations with no remaining data\n",
    "    if station_df.empty:\n",
    "        continue\n",
    "\n",
    "    # Write the infos above in 'stationlist'\n",
    "    sta_miss.append({\n",
    "            \"station\": station_df[\"station\"].iloc[0],\n",
    "            \"startdate\": station_df[\"wdsp\"].isna().sum()\n",
    "        })\n",
    "    \n",
    "# Save stationinfo.csv\n",
    "sta_miss_df = pd.DataFrame(sta_miss)\n",
    "    # Save as a new file in folder stationdata\n",
    "    #station_df.to_csv(file, index=False)\n",
    "print(sta_miss_df)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f2f85aa4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('datestudy.csv')\n",
    "pd.api.types.is_datetime64_any_dtype(df[\"date\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "956aa0ad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.int64(24)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# change the type on 'wdsp' to numeric, the missing value will be turned into NaN with parameter errors=\"coerce\"\n",
    "df[\"wdsp\"] = pd.to_numeric(df[\"wdsp\"], errors=\"coerce\")\n",
    "\n",
    "# count the number of missing values\n",
    "df[\"wdsp\"].isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "6f2c468e",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "\"None of ['date'] are in the columns\"",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyError\u001b[39m                                  Traceback (most recent call last)",
      "\u001b[32m/tmp/ipykernel_3248/2510646409.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Set 'date' as index as interpolate based on actual time differences between index values\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m df = df.set_index(\u001b[33m'date'\u001b[39m)\n\u001b[32m      3\u001b[39m \n\u001b[32m      4\u001b[39m \u001b[38;5;66;03m# Interpolate missing windspeed (linear is best for meteorological data) -- I use AI to help me with this.\u001b[39;00m\n\u001b[32m      5\u001b[39m df[\u001b[33m'wdsp'\u001b[39m] = df[\u001b[33m'wdsp'\u001b[39m].interpolate(method=\u001b[33m'time'\u001b[39m, limit_direction=\u001b[33m'both'\u001b[39m, limit_area=\u001b[33m'inside'\u001b[39m)\n",
      "\u001b[32m~/.local/lib/python3.12/site-packages/pandas/core/frame.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(self, keys, drop, append, inplace, verify_integrity)\u001b[39m\n\u001b[32m   6140\u001b[39m                     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28;01mnot\u001b[39;00m found:\n\u001b[32m   6141\u001b[39m                         missing.append(col)\n\u001b[32m   6142\u001b[39m \n\u001b[32m   6143\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m missing:\n\u001b[32m-> \u001b[39m\u001b[32m6144\u001b[39m             \u001b[38;5;28;01mraise\u001b[39;00m KeyError(f\"None of {missing} are in the columns\")\n\u001b[32m   6145\u001b[39m \n\u001b[32m   6146\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m inplace:\n\u001b[32m   6147\u001b[39m             frame = self\n",
      "\u001b[31mKeyError\u001b[39m: \"None of ['date'] are in the columns\""
     ]
    }
   ],
   "source": [
    "# Set 'date' as index as interpolate based on actual time differences between index values\n",
    "df = df.set_index('date')\n",
    "\n",
    "# Interpolate missing windspeed (linear is best for meteorological data) -- I use AI to help me with this.\n",
    "df['wdsp'] = df['wdsp'].interpolate(method='time', limit_direction='both', limit_area='inside')\n",
    "\n",
    "# count the number of missing values\n",
    "#df[\"wdsp\"].isna().sum()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
